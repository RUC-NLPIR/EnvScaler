"""
Utility functions for evaluation.
"""
def attribute_checker(
    model_output: dict,
    possible_answer: dict,
):
    """
    Model output checker, used to compare the attribute values of the class instance generated by the Agent
    with the possible correct answer, to determine if they are consistent.
    Traverse each attribute generated by the model:
        If the expected answer does not have this attribute → error (missing)
        If it is a dictionary type (multi-level nested) → traverse the nested key-value pairs one by one to compare
            If the key or value is missing → error
        If it is a single layer value → compare if they are equal
    """
    # Initialize result dictionary
    result = {
        "valid": True,
        "error": [],
        "error_type": "class attributes wrong",
    }

    # Extract scenario_name
    scenario_name = list(possible_answer.keys())[0]
    possible_answer = list(possible_answer.values())[0]
    model_params = list(model_output.values())[0]


    # Traverse each attribute name and value in the model output
    for model_param, model_value in model_params.items():
        # Get the corresponding possible_answer_value
        if model_param in possible_answer:
            # If the attribute exists in the expected answer
            possible_answer_value = possible_answer[model_param]
        else:
            # If the attribute does not exist in the expected answer, determine an error
            result["valid"] = False
            result["error"].append(f"class({scenario_name}) attributes({model_param}) missing in possible_answer.")
            continue

        # If the expected value is a dictionary type (indicates a nested attribute)
        if isinstance(possible_answer_value, dict):
            # Compare each nested key-value pair
            for param, value in possible_answer_value.items():
                if param not in model_value:
                    result["valid"] = False
                    result["error"].append(
                        f"class({scenario_name}) attributes({model_param}) wrong, [expected: {possible_answer_value}, real: {model_value}]"
                    )
                elif value != model_value[param]:
                    result["valid"] = False
                    result["error"].append(
                        f"class({scenario_name}) attributes({model_param}.{param}) wrong, [expected: {value}, real: {model_value[param]}]"
                    )
        else:
            # If the expected value is not a dictionary, compare if the values are equal
            if possible_answer_value != model_value:
                result["valid"] = False
                result["error"].append(
                    f"class({scenario_name}) attributes({model_param}) wrong, [expected: {possible_answer_value}, real: {model_value}]"
                )

    return result

def calculate_process_accuracy(call_process_item, model_result):
    """
    Calculate the accuracy of a call_process_item.
    call_process_item: list[str] represents a milestone sequence
    eg.
    [
        "[turn_on_wifi()]",
        "[get_latest_message_id()]",
        "[delete_message(message_id=4)]",
        "[send_message(sender_name=\'Eve\', receiver_name=\'Frank\', message=\'Don\'t forget tomorrow afternoon\'s meeting\')]"
    ]
    model_result: list[str] represents the sequence of model outputs
    # eg
    [
        "[send_message(sender_name=\'Eve\', receiver_name=\'Frank\', message=\\"Don\'t forget tomorrow afternoon\'s meeting\\")]",
        "[turn_on_wifi()]",
        "[send_message(sender_name=\'Eve\', receiver_name=\'Frank\', message=\\"Don\'t forget tomorrow afternoon\'s meeting\\")]",
        "view_messages_between_users(sender_name=\'Eve\', receiver_name=\'Frank\')",
        "[delete_message(message_id=1)]",
        "[send_message(sender_name=\'Eve\', receiver_name=\'Frank\', message=\\"Don\'t forget tomorrow afternoon\'s meeting\\")]"
    ]
    Returns: (rounded_accuracy, accuracy, result_indices)
    """
    result_len = len(model_result)
    milestone_len = len(call_process_item)
    result_indices = []
    current_index = 0  

    for stone in call_process_item:
        # Sequential search for matching
        while current_index < result_len:
            if model_result[current_index].strip() == stone.strip():
                result_indices.append(current_index)
                current_index += 1  
                break
            current_index += 1
    
    if milestone_len == 0:
        accuracy = 1.00
    else:
        accuracy = len(result_indices) / milestone_len

    rounded_accuracy = round(accuracy, 3)
    return rounded_accuracy, accuracy, result_indices