defaults:
  - ../config/deepspeed_zero@_here_
  - ../config/deepspeed_zero2@_here_
  - ../config/deepspeed_zero3@_here_
  - ../config/deepspeed_zero3_cpuoffload@_here_

hydra:
  run:
    dir: .
  output_subdir: null

# ------------------------------------------------
# hyperparameters setting
exp_name: envscaler_non_conv_rl
# model_name: Qwen/Qwen3-8B
model_name: your_qwen3_8b_sft_model_path

num_gpus_per_node: 8

max_steps: 201
save_steps: 50
logging_steps: 1
eval_steps: 5

# multi-turn related
sequence_length: 32000
max_actions_per_traj: 40
max_token_per_step: 4096

# batch size related
rollout_batch_size: 512  # rollout_batch_size = num_env_groups Ã— group_size
val_batch_size: 128
per_device_train_batch_size: 1
gradient_accumulation_steps: 64
actor_infer_batch_size: 1
reference_infer_batch_size: 1
train_group_size: 8 # number of rollout for each sample
train_num_env_groups: 64 # number of independent samples in rollout

# other
reference_kl_coef: 0.1

# ------------------------------------------------

seed: 42
logging_dir: /your_output_dir/${exp_name}_${now:%Y%m%d_%H%M%S}/output/logs
output_dir: /your_output_dir/${exp_name}_${now:%Y%m%d_%H%M%S}/output
render_save_dir: /your_output_dir/${exp_name}_${now:%Y%m%d_%H%M%S}/output/render
traj_save_dir: /your_output_dir/${exp_name}_${now:%Y%m%d_%H%M%S}/output/rollouts
system_envs:
  USE_MODELSCOPE: '1'


track_with: wandb
tracker_kwargs:
  api_key:
  project: env_scaler
  name: ${exp_name}
  notes: "agentic_pipeline"
  tags:
    - qwen3


checkpoint_config:
  type: file_system
  output_dir: /your_output_dir/${exp_name}_${now:%Y%m%d_%H%M%S}/checkpoint


resume_from_checkpoint: false

advantage_clip: 0.2
ppo_epochs: 1
adv_estimator: "reinforce"
init_kl_coef: ${reference_kl_coef}
whiten_advantages: true
entropy_loss_coef: 0
max_grad_norm: 1.0

pretrain: ${model_name}
reward_pretrain: ${model_name}

actor_train:
  model_args:
    attn_implementation: fa2
    disable_gradient_checkpointing: false
    dtype: bf16
    model_type: ~
  training_args:
    learning_rate: 1.0e-6
    weight_decay: 0
    per_device_train_batch_size: ${per_device_train_batch_size}
    gradient_accumulation_steps: ${gradient_accumulation_steps}
    warmup_steps: 10
    lr_scheduler_type: cosine
  data_args:
    template: qwen3
  strategy_args:
    strategy_name: megatron_train
    strategy_config:
      tensor_model_parallel_size: 2
      pipeline_model_parallel_size: 1
      expert_model_parallel_size: 1
      use_distributed_optimizer: true
      recompute_granularity: full
  device_mapping: list(range(0,8))
  infer_batch_size: ${actor_infer_batch_size}

actor_infer:
  model_args:
    disable_gradient_checkpointing: true
    dtype: bf16
  generating_args:
    max_new_tokens: ${max_token_per_step} # single-turn response length
    top_p: 0.99
    top_k: 100
    num_beams: 1
    temperature: 0.99
    num_return_sequences: 1
  data_args:
    template: qwen3
  strategy_args:
    strategy_name: vllm
    strategy_config:
      gpu_memory_utilization: 0.8
      block_size: 16
      load_format: auto
  device_mapping: list(range(0,8))

reference:
  model_args:
    attn_implementation: fa2
    disable_gradient_checkpointing: true
    dtype: bf16
    model_type: ~
  data_args:
    template: qwen3
  strategy_args:
    strategy_name: megatron_infer
    strategy_config:
      tensor_model_parallel_size: 2
      pipeline_model_parallel_size: 1
      expert_model_parallel_size: 1
      use_distributed_optimizer: true
      recompute_granularity: full
  device_mapping: list(range(0,8))
  infer_batch_size: ${reference_infer_batch_size}

reward_normalization:
  grouping: traj_group_id
  method: mean_std

# num_env_groups = sum(num_groups_partition)
# tags and num_groups_partition correspond one-to-one

train_env_manager:
  format_penalty: 0
  max_env_num_per_worker: 8
  num_env_groups: ${train_num_env_groups}
  group_size: ${train_group_size}
  tags: [SynEnvNonConversationTrain]
  num_groups_partition: [64]

val_env_manager:
  max_env_num_per_worker: 8
  num_env_groups: 128
  group_size: 1 # should be set to 1 because val temperature is set to 0 and same prompt leads to same output
  tags: [BFCLEval]
  num_groups_partition: [128]


custom_envs:
  SynEnvNonConversationTrain:
    env_type: envscaler_non_conv_env
    max_steps: ${max_actions_per_traj}
    max_tokens_per_step: ${max_token_per_step}
    env_manager_cls: roll.pipeline.agentic.env_manager.traj_env_manager_for_env_scaler.TrajEnvManager
    env_config:
      mode: train

  SynEnvConversationTrain:
    env_type: envscaler_conv_env
    max_steps: ${max_actions_per_traj}
    max_tokens_per_step: ${max_token_per_step}
    env_manager_cls: roll.pipeline.agentic.env_manager.traj_env_manager_for_env_scaler.TrajEnvManager
    env_config:
      mode: train
      user_model: gpt-4.1  
      provider: openai

  BFCLEval:
    env_type: bfcl
    max_steps: ${max_actions_per_traj}
    max_tokens_per_step: ${max_token_per_step}
    env_manager_cls: roll.pipeline.agentic.env_manager.traj_env_manager_for_env_scaler.TrajEnvManager
    env_config:
      mode: multi_turn_base